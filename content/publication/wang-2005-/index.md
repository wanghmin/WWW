---
title: Towards space-time light field rendering
authors:
- Huamin Wang
- Ruigang Yang
date: '2005-01-01'
publishDate: '2024-02-28T12:02:47.901045Z'
publication_types:
- paper-conference
publication: '*Proceedings of the 2005 Symposium on Interactive 3D Graphics and Games*'
doi: 10.1145/1053427.1053448
abstract: So far extending light field rendering to dynamic scenes has been trivially
  treated as the rendering of static light fields stacked in time. This type of approaches
  requires input video sequences in strict synchronization and allows only discrete
  exploration in the temporal domain determined by the capture rate. In this paper
  we propose a novel framework, space-time light field rendering, which allows continuous
  exploration of a dynamic scene in both spatial and temporal domain with unsynchronized
  input video sequences.In order to synthesize novel views from any viewpoint at any
  time instant, we develop a two-stage rendering algorithm. We first interpolate in
  the temporal domain to generate globally synchronized images using a robust spatial-temporal
  image registration algorithm followed by edge-preserving image morphing. We then
  interpolate those software-synchronized images in the spatial domain to synthesize
  the final view. Our experimental results show that our approach is robust and capable
  of maintaining photo-realistic results.
summary: So far extending light field rendering to dynamic scenes has been trivially
  treated as the rendering of static light fields stacked in time. This type of approaches
  requires input video sequences in strict synchronization and allows only discrete
  exploration in the temporal domain determined by the capture rate. In this paper
  we propose a novel framework, space-time light field rendering, which allows continuous
  exploration of a dynamic scene in both spatial and temporal domain with unsynchronized
  input video sequences.In order to synthesize novel views from any viewpoint at any
  time instant, we develop a two-stage rendering algorithm. We first interpolate in
  the temporal domain to generate globally synchronized images using a robust spatial-temporal
  image registration algorithm followed by edge-preserving image morphing. We then
  interpolate those software-synchronized images in the spatial domain to synthesize
  the final view. Our experimental results show that our approach is robust and capable
  of maintaining photo-realistic results.
tags:
- space-time light field
- image-based rendering
- epipolar constraints
links:
- name: URL
  url: https://doi.org/10.1145/1053427.1053448
url_pdf: ''
url_code: 'https://github.com/HugoBlox/hugo-blox-builder'
url_dataset: 'https://github.com/HugoBlox/hugo-blox-builder'
url_poster: ''
url_project: ''
url_slides: ''
url_source: 'https://github.com/HugoBlox/hugo-blox-builder'
url_video: 'https://youtube.com'
---
