@inproceedings{Wang:2005:TST,
	author = {Wang, Huamin and Yang, Ruigang},
	title = {Towards space-time light field rendering},
	year = {2005},
	isbn = {1595930132},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/1053427.1053448},
	doi = {10.1145/1053427.1053448},
	abstract = {So far extending light field rendering to dynamic scenes has been trivially treated as the rendering of static light fields stacked in time. This type of approaches requires input video sequences in strict synchronization and allows only discrete exploration in the temporal domain determined by the capture rate. In this paper we propose a novel framework, space-time light field rendering, which allows continuous exploration of a dynamic scene in both spatial and temporal domain with unsynchronized input video sequences. In order to synthesize novel views from any viewpoint at any time instant, we develop a two-stage rendering algorithm. We first interpolate in the temporal domain to generate globally synchronized images using a robust spatial-temporal image registration algorithm followed by edge-preserving image morphing. We then interpolate those software-synchronized images in the spatial domain to synthesize the final view. Our experimental results show that our approach is robust and capable of maintaining photo-realistic results.},
	booktitle = {Proceedings of the 2005 Symposium on Interactive 3D Graphics and Games (I3D)},
	pages = {125--132},
	numpages = {8},
	keywords = {space-time light field, image-based rendering, epipolar constraints},
	location = {Washington, District of Columbia},
	series = {I3D '05}
}

@article{Wang:2005:WDS,
	author = {Wang, Huamin and Mucha, Peter J. and Turk, Greg},
	title = {Water drops on surfaces},
	year = {2005},
	issue_date = {July 2005},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {24},
	number = {3},
	issn = {0730-0301},
	url = {https://doi.org/10.1145/1073204.1073284},
	doi = {10.1145/1073204.1073284},
	abstract = {We present a physically-based method to enforce contact angles at the intersection of fluid free surfaces and solid objects, allowing us to simulate a variety of small-scale fluid phenomena including water drops on surfaces. The heart of this technique is a virtual surface method, which modifies the level set distance field representing the fluid surface in order to maintain an appropriate contact angle. The surface tension that is calculated on the contact line between the solid surface and liquid surface can then capture all interfacial tensions, including liquid-solid, liquid-air and solid-air tensions. We use a simple dynamic contact angle model to select contact angles according to the solid material property, water history, and the fluid front's motion. Our algorithm robustly and accurately treats various drop shape deformations, and handles both flat and curved solid surfaces. Our results show that our algorithm is capable of realistically simulating several small-scale liquid phenomena such as beading and flattened drops, stretched and separating drops, suspended drops on curved surfaces, and capillary action.},
	journal = {ACM Trans. Graph. (SIGGRAPH)},
	month = {jul},
	pages = {921--929},
	numpages = {9},
	keywords = {contact line/angle, liquid-solid interaction, physically based animation, virtual surface, water drop}
}

@inproceedings{Wang:2007:SGS,
	author = {Wang, Huamin and Miller, Gavin and Turk, Greg},
	title = {Solving general shallow wave equations on surfaces},
	year = {2007},
	isbn = {9781595936240},
	publisher = {Eurographics Association},
	address = {Goslar, DEU},
	abstract = {We propose a new framework for solving General Shallow Wave Equations (GSWE) in order to efficiently simulate water flows on solid surfaces under shallow wave assumptions. Within this framework, we develop implicit schemes for solving the external forces applied to water, including gravity and surface tension. We also present a two-way coupling method to model interactions between fluid and floating rigid objects. Water flows in this system can be simulated not only on planar surfaces by using regular grids, but also on curved surfaces directly without surface parametrization. The experiments show that our system is fast, stable, physically sound, and straightforward to implement on both CPUs and GPUs. It is capable of simulating a variety of water effects including: shallow waves, water drops, rivulets, capillary events and fluid/floating rigid body coupling. Because the system is fast, we can also achieve real-time water drop control and shape design.},
	booktitle = {Proceedings of the 2007 ACM SIGGRAPH/Eurographics Symposium on Computer Animation (SCA)},
	pages = {229--238},
	numpages = {10},
	location = {San Diego, California},
	series = {SCA '07}
}

@ARTICLE{Wang:2007:STL,
	author={Wang, Huamin and Sun, Mingxuan and Yang, Ruigang},
	journal={IEEE Transactions on Visualization and Computer Graphics}, 
	title={Space-Time Light Field Rendering}, 
	month={jul},
	year={2007},
	volume={13},
	number={4},
	pages={697--710},
	keywords={Layout;Robustness;Motion control;Lighting control;Control systems;Visualization;Control system synthesis;Rendering (computer graphics);Image generation;Image registration;Image-based rendering;space-time light field;epipolar constraint;image morphing},	
	abstract={In this paper, we propose a novel framework called space-time light field rendering, which allows continuous exploration of a dynamic scene in both space and time. Compared to existing light field capture/rendering systems, it offers the capability of using unsynchronized video inputs and the added freedom of controlling the visualization in the temporal domain, such as smooth slow motion and temporal integration. In order to synthesize novel views from any viewpoint at any time instant, we develop a two-stage rendering algorithm. We first interpolate in the temporal domain to generate globally synchronized images using a robust spatial-temporal image registration algorithm followed by edge-preserving image morphing. We then interpolate these software-synchronized images in the spatial domain to synthesize the final view. In addition, we introduce a very accurate and robust algorithm to estimate subframe temporal offsets among input video sequences. Experimental results from unsynchronized videos with or without time stamps show that our approach is capable of maintaining photorealistic quality from a variety of real scenes.},	
	doi={10.1109/TVCG.2007.1019}
}

@article{Wang:2008:FRC,
	author = {Wang, Huamin and Wexler, Yonatan and Ofek, Eyal and Hoppe, Hugues},
	title = {Factoring repeated content within and among images},
	year = {2008},
	issue_date = {August 2008},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {27},
	number = {3},
	issn = {0730-0301},
	url = {https://doi.org/10.1145/1360612.1360613},
	doi = {10.1145/1360612.1360613},
	abstract = {We reduce transmission bandwidth and memory space for images by factoring their repeated content. A transform map and a condensed epitome are created such that all image blocks can be reconstructed from transformed epitome patches. The transforms may include affine deformation and color scaling to account for perspective and tonal variations across the image. The factored representation allows efficient random-access through a simple indirection, and can therefore be used for real-time texture mapping without expansion in memory. Our scheme is orthogonal to traditional image compression, in the sense that the epitome is amenable to further compression such as DXT. Moreover it allows a new mode of progressivity, whereby generic features appear before unique detail. Factoring is also effective across a collection of images, particularly in the context of image-based rendering. Eliminating redundant content lets us include textures that are several times as large in the same memory space.},
	journal = {ACM Trans. Graph. (SIGGRAPH)},
	month = {aug},
	pages = {1?10},
	numpages = {10},
	keywords = {image compression, image epitomes, progressive images}
}
